{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark DataFrames, Schemas, and Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, FloatType, DateType, BooleanType\n",
    "from pyspark.sql.functions import col, expr, concat_ws, round\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/27 09:26:19 WARN Utils: Your hostname, Cesars-MBP.local resolves to a loopback address: 127.0.0.1; using 192.168.7.230 instead (on interface en0)\n",
      "25/02/27 09:26:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/27 09:26:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"PySparkDataFrames\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tuples\n",
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StructType\n",
    "\n",
    "A `StructType()` is a collection of `StructField()` that define the column's name, data type, and a boolean value to specify if the field can have NULL values or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"firstName\", StringType(), True),\n",
    "    StructField(\"middleName\", StringType(), True),\n",
    "    StructField(\"lastName\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- middleName: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstName|middleName|lastName|id   |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|James    |          |Smith   |36636|M     |3000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Reader and Writer\n",
    "\n",
    "The DataFrame Reader is a built-in API within the DataFrame that allows one to read various source files (CSV, JSON) and other Big Data file types (Parquet, ORC, and AVRO). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../data/input/fire-incidents.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fire_incidents_df = (spark.read.format(\"csv\")\n",
    "                     .option(\"header\", True)\n",
    "                     .option(\"inferSchema\", True)\n",
    "                     .load(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+-------------+\n",
      "|IncidentNumber|       IncidentDate|         City|\n",
      "+--------------+-------------------+-------------+\n",
      "|      20104668|2020-09-11 00:00:00|San Francisco|\n",
      "|      20104708|2020-09-11 00:00:00|San Francisco|\n",
      "|      20104648|2020-09-10 00:00:00|San Francisco|\n",
      "|      20104598|2020-09-10 00:00:00|San Francisco|\n",
      "|      20104575|2020-09-10 00:00:00|San Francisco|\n",
      "|      20104477|2020-09-10 00:00:00|San Francisco|\n",
      "|      20104443|2020-09-10 00:00:00|San Francisco|\n",
      "|      20104605|2020-09-10 00:00:00|San Francisco|\n",
      "|      20104474|2020-09-10 00:00:00|San Francisco|\n",
      "|      20104652|2020-09-10 00:00:00|San Francisco|\n",
      "+--------------+-------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_incidents_df.select(\"IncidentNumber\", \"IncidentDate\", \"City\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note***: The `select()` statement refers to a *projection*, which projects (selects) the columns one requires. Spark will then resolve at the schema level after an action is called. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- ExposureNumber: integer (nullable = true)\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- IncidentDate: timestamp (nullable = true)\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- AlarmDtTm: timestamp (nullable = true)\n",
      " |-- ArrivalDtTm: timestamp (nullable = true)\n",
      " |-- CloseDtTm: timestamp (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- ZIPCode: string (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- SuppressionUnits: integer (nullable = true)\n",
      " |-- SuppressionPersonnel: integer (nullable = true)\n",
      " |-- EMSUnits: integer (nullable = true)\n",
      " |-- EMSPersonnel: integer (nullable = true)\n",
      " |-- OtherUnits: integer (nullable = true)\n",
      " |-- OtherPersonnel: integer (nullable = true)\n",
      " |-- FirstUnitOnScene: string (nullable = true)\n",
      " |-- EstimatedPropertyLoss: integer (nullable = true)\n",
      " |-- EstimatedContentsLoss: double (nullable = true)\n",
      " |-- FireFatalities: integer (nullable = true)\n",
      " |-- FireInjuries: integer (nullable = true)\n",
      " |-- CivilianFatalities: integer (nullable = true)\n",
      " |-- CivilianInjuries: integer (nullable = true)\n",
      " |-- NumberofAlarms: integer (nullable = true)\n",
      " |-- PrimarySituation: string (nullable = true)\n",
      " |-- MutualAid: string (nullable = true)\n",
      " |-- ActionTakenPrimary: string (nullable = true)\n",
      " |-- ActionTakenSecondary: string (nullable = true)\n",
      " |-- ActionTakenOther: string (nullable = true)\n",
      " |-- DetectorAlertedOccupants: string (nullable = true)\n",
      " |-- PropertyUse: string (nullable = true)\n",
      " |-- AreaofFireOrigin: string (nullable = true)\n",
      " |-- IgnitionCause: string (nullable = true)\n",
      " |-- IgnitionFactorPrimary: string (nullable = true)\n",
      " |-- IgnitionFactorSecondary: string (nullable = true)\n",
      " |-- HeatSource: string (nullable = true)\n",
      " |-- ItemFirstIgnited: string (nullable = true)\n",
      " |-- HumanFactorsAssociatedwithIgnition: string (nullable = true)\n",
      " |-- StructureType: string (nullable = true)\n",
      " |-- StructureStatus: string (nullable = true)\n",
      " |-- FloorofFireOrigin: integer (nullable = true)\n",
      " |-- FireSpread: string (nullable = true)\n",
      " |-- NoFlameSpead: string (nullable = true)\n",
      " |-- Numberoffloorswithminimumdamage: integer (nullable = true)\n",
      " |-- Numberoffloorswithsignificantdamage: integer (nullable = true)\n",
      " |-- Numberoffloorswithheavydamage: integer (nullable = true)\n",
      " |-- Numberoffloorswithextremedamage: integer (nullable = true)\n",
      " |-- DetectorsPresent: string (nullable = true)\n",
      " |-- DetectorType: string (nullable = true)\n",
      " |-- DetectorOperation: string (nullable = true)\n",
      " |-- DetectorEffectiveness: string (nullable = true)\n",
      " |-- DetectorFailureReason: string (nullable = true)\n",
      " |-- AutomaticExtinguishingSystemPresent: string (nullable = true)\n",
      " |-- AutomaticExtinguishingSytemType: string (nullable = true)\n",
      " |-- AutomaticExtinguishingSytemPerfomance: string (nullable = true)\n",
      " |-- AutomaticExtinguishingSytemFailureReason: string (nullable = true)\n",
      " |-- NumberofSprinklerHeadsOperating: integer (nullable = true)\n",
      " |-- SupervisorDistrict: integer (nullable = true)\n",
      " |-- AnalysisNeighborhood: string (nullable = true)\n",
      " |-- point: string (nullable = true)\n",
      " |-- NeighborhoodsOld: integer (nullable = true)\n",
      " |-- ZipCodes: integer (nullable = true)\n",
      " |-- FirePreventionDistricts: integer (nullable = true)\n",
      " |-- PoliceDistricts: integer (nullable = true)\n",
      " |-- SupervisorDistricts: integer (nullable = true)\n",
      " |-- CivicCenterHarmReductionProjectBoundary: integer (nullable = true)\n",
      " |-- 2017FixItZones: integer (nullable = true)\n",
      " |-- HSOCZones: integer (nullable = true)\n",
      " |-- CentralMarketTenderloinBoundary: integer (nullable = true)\n",
      " |-- CentralMarketTenderloinBoundaryPolygonUpdated: integer (nullable = true)\n",
      " |-- HSOCZonesasof20180605: integer (nullable = true)\n",
      " |-- Neighborhoods: integer (nullable = true)\n",
      " |-- SFFindNeighborhoods: integer (nullable = true)\n",
      " |-- CurrentPoliceDistricts: integer (nullable = true)\n",
      " |-- CurrentSupervisorDistricts: integer (nullable = true)\n",
      " |-- AnalysisNeighborhoods: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_incidents_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a datetime stamp to the directory name that will contain the collection of PARQUET files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/27 09:27:20 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/27 09:27:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "export_path = f\"../../data/output/fire_incidents_{date_timestamp}\"\n",
    "\n",
    "fire_incidents_df.write.format(\"parquet\").mode(\"overwrite\").save(export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_json_persons = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"favorite_movies\", ArrayType(StringType()), True), # Note the StringType inside ArrayType\n",
    "    StructField(\"salary\", FloatType(), True), \n",
    "    StructField(\"image_url\", StringType(), True), \n",
    "    StructField(\"date_of_birth\", DateType(), True), \n",
    "    StructField(\"active\", BooleanType(), True),  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"../../data/input/persons.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons_df = spark.read.json(json_file_path, schema=schema_json_persons, multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- favorite_movies: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- image_url: string (nullable = true)\n",
      " |-- date_of_birth: date (nullable = true)\n",
      " |-- active: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persons_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+---------------+-------+-----------------------------------------------+-------------+------+\n",
      "|id |first_name|last_name|favorite_movies|salary |image_url                                      |date_of_birth|active|\n",
      "+---+----------+---------+---------------+-------+-----------------------------------------------+-------------+------+\n",
      "|1  |Drucy     |Poppy    |null           |1463.36|http://dummyimage.com/126x166.png/cc0000/ffffff|1991-02-16   |true  |\n",
      "|2  |Emelyne   |Blaza    |null           |3006.04|http://dummyimage.com/158x106.bmp/cc0000/ffffff|1991-11-02   |false |\n",
      "|3  |Max       |Rettie   |null           |1422.88|http://dummyimage.com/237x140.jpg/ff4444/ffffff|1990-03-03   |false |\n",
      "|4  |Ilario    |Kean     |null           |3561.36|http://dummyimage.com/207x121.jpg/cc0000/ffffff|1987-06-09   |true  |\n",
      "|5  |Toddy     |Drexel   |null           |4934.87|http://dummyimage.com/116x202.png/cc0000/ffffff|1992-10-28   |true  |\n",
      "|6  |Oswald    |Petrolli |null           |1153.23|http://dummyimage.com/137x172.jpg/5fa2dd/ffffff|1986-09-02   |false |\n",
      "|7  |Adrian    |Clarey   |null           |1044.73|http://dummyimage.com/244x218.bmp/cc0000/ffffff|1971-08-24   |false |\n",
      "|8  |Dominica  |Goodnow  |null           |1147.76|http://dummyimage.com/112x203.jpg/dddddd/000000|1973-08-27   |false |\n",
      "|9  |Emory     |Slocomb  |null           |1082.11|http://dummyimage.com/138x226.jpg/cc0000/ffffff|1974-06-08   |true  |\n",
      "|10 |Jeremias  |Bode     |null           |3472.63|http://dummyimage.com/243x108.bmp/dddddd/000000|1997-08-02   |true  |\n",
      "+---+----------+---------+---------------+-------+-----------------------------------------------+-------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persons_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns and Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|first_name|last_name|date_of_birth|\n",
      "+----------+---------+-------------+\n",
      "|     Drucy|    Poppy|   1991-02-16|\n",
      "|   Emelyne|    Blaza|   1991-11-02|\n",
      "|       Max|   Rettie|   1990-03-03|\n",
      "|    Ilario|     Kean|   1987-06-09|\n",
      "|     Toddy|   Drexel|   1992-10-28|\n",
      "+----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persons_df.select(\"first_name\", \"last_name\", \"date_of_birth\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|first_name|last_name|date_of_birth|\n",
      "+----------+---------+-------------+\n",
      "|     Drucy|    Poppy|   1991-02-16|\n",
      "|   Emelyne|    Blaza|   1991-11-02|\n",
      "|       Max|   Rettie|   1990-03-03|\n",
      "|    Ilario|     Kean|   1987-06-09|\n",
      "|     Toddy|   Drexel|   1992-10-28|\n",
      "+----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persons_df.select(col(\"first_name\"), col(\"last_name\"), col(\"date_of_birth\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note***: There is no noticeable difference between the two outputs, but there is a difference:\n",
    "- In the first example, one passes the column names as strings. Spark converts these strings into column objects which is concise and works wells for simple column selections. \n",
    "- By using the `col()` function, one explicitly creates a column object for each specified column name, which is more flexible when wanting to perform further transformations on the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+------------------+\n",
      "|       full_name| salary|   salary_increase|\n",
      "+----------------+-------+------------------+\n",
      "|     Drucy Poppy|1463.36|1609.6959838867188|\n",
      "|   Emelyne Blaza|3006.04|  3306.64404296875|\n",
      "|      Max Rettie|1422.88|1565.1680053710938|\n",
      "|     Ilario Kean|3561.36|3917.4961181640624|\n",
      "|    Toddy Drexel|4934.87|  5428.35712890625|\n",
      "| Oswald Petrolli|1153.23| 1268.552978515625|\n",
      "|   Adrian Clarey|1044.73| 1149.202978515625|\n",
      "|Dominica Goodnow|1147.76|1262.5360107421875|\n",
      "|   Emory Slocomb|1082.11|1190.3209838867188|\n",
      "|   Jeremias Bode|3472.63|  3819.89287109375|\n",
      "+----------------+-------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(persons_df.select(concat_ws(\" \", col(\"first_name\"), col(\"last_name\")).alias(\"full_name\"),\n",
    "                   col(\"salary\"),\n",
    "                   (col(\"salary\") * 0.10 + col(\"salary\")).alias(\"salary_increase\"))).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+------------------+\n",
      "|       full_name| salary|   salary_increase|\n",
      "+----------------+-------+------------------+\n",
      "|     Drucy Poppy|1463.36|1609.6959838867188|\n",
      "|   Emelyne Blaza|3006.04|  3306.64404296875|\n",
      "|      Max Rettie|1422.88|1565.1680053710938|\n",
      "|     Ilario Kean|3561.36|3917.4961181640624|\n",
      "|    Toddy Drexel|4934.87|  5428.35712890625|\n",
      "| Oswald Petrolli|1153.23| 1268.552978515625|\n",
      "|   Adrian Clarey|1044.73| 1149.202978515625|\n",
      "|Dominica Goodnow|1147.76|1262.5360107421875|\n",
      "|   Emory Slocomb|1082.11|1190.3209838867188|\n",
      "|   Jeremias Bode|3472.63|  3819.89287109375|\n",
      "+----------------+-------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(persons_df.select(concat_ws(\" \", col(\"first_name\"), col(\"last_name\")).alias(\"full_name\"),\n",
    "                   col(\"salary\"),\n",
    "                   expr(\"salary * 0.10 + salary\").alias(\"salary_increase\"))).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note***: The outputs are identical, but differ by applying different functions to each approach.\n",
    "- With the `concat_ws()` and `alias()` functions, one concatenates two columns into a new feature, and adds another feature based on the calculation of a column, where the added features are given an alias. \n",
    "- By using the `expr()`, the code in the second example is much cleaner and easier to apply the transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+---------------+\n",
      "|       full_name| salary|salary_increase|\n",
      "+----------------+-------+---------------+\n",
      "|     Drucy Poppy|1463.36|         1609.7|\n",
      "|   Emelyne Blaza|3006.04|        3306.64|\n",
      "|      Max Rettie|1422.88|        1565.17|\n",
      "|     Ilario Kean|3561.36|         3917.5|\n",
      "|    Toddy Drexel|4934.87|        5428.36|\n",
      "| Oswald Petrolli|1153.23|        1268.55|\n",
      "|   Adrian Clarey|1044.73|         1149.2|\n",
      "|Dominica Goodnow|1147.76|        1262.54|\n",
      "|   Emory Slocomb|1082.11|        1190.32|\n",
      "|   Jeremias Bode|3472.63|        3819.89|\n",
      "+----------------+-------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(persons_df.select(concat_ws(\" \", col(\"first_name\"), col(\"last_name\")).alias(\"full_name\"),\n",
    "                   col(\"salary\"),\n",
    "                   round(expr(\"salary * 0.10 + salary\"), 2).alias(\"salary_increase\"))).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note***: Applied rounding to the salary_increase column. Not necessary for data engineering purposes, but useful for data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Where Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
