{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdcce9da-dba4-4b3a-828b-89258e9a83b1",
   "metadata": {},
   "source": [
    "# Introduction to PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c15b268-3f4b-4bf5-8685-c568727e6cef",
   "metadata": {},
   "source": [
    "## Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48159c40-3092-4cd4-a857-fa0d4b6f69f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78048e0-7da0-4b23-be54-a1ec9374cd63",
   "metadata": {},
   "source": [
    "## Spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7893ba-0aff-445f-bcf7-f9438fdb6b54",
   "metadata": {},
   "source": [
    "In **PySpark**, ```SparkContext``` serves as the entry point to Spark functionality. It represents the connection to a Spark cluster\n",
    "and is used to coordinate and manage the execution of Spark applications. It also handles the communication with the cluster manager to allocate resources and schedule tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5911fa9a-30ea-4ae4-bfc0-729b787c4834",
   "metadata": {},
   "source": [
    "In this case, we will connect to a local cluster with by creating as many possible worker threads on logical cores (CPU) to run the Spark the job in parallel with the ```local[*]``` syntax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "203f00a2-2cf6-4416-b7ed-3f28e69ffc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/13 11:52:47 WARN Utils: Your hostname, Cesars-MBP.local resolves to a loopback address: 127.0.0.1; using 192.168.7.230 instead (on interface en0)\n",
      "25/02/13 11:52:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/13 11:52:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba98970e-fffe-47fa-88eb-d52086da52d5",
   "metadata": {},
   "source": [
    "## Resilient Distributed Dataset (RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bbb169-ba35-4f8a-8f3d-d4eafa8a0c45",
   "metadata": {},
   "source": [
    "In **PySpark**, RDD is a fundamental data structure and abstraction that represents an immutable, distributed collection of data elements.\n",
    "\n",
    "RDDs provide a powerful abstraction for distributed data processing. They handle the complexities of distributing data, fault tolerance, and parallel execution, allowing developers to focus on the logic of their data processing tasks. While newer abstractions like DataFrames and Datasets are built on top of RDDs and offer more structured data handling and optimizations, understanding RDDs is still crucial for a deeper understanding of Spark's internals and for certain advanced use cases.  They are the foundation upon which Spark's more modern data processing capabilities are built."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b39c839-fa51-4750-9ebb-28eef06dabff",
   "metadata": {},
   "source": [
    "Let's break down what each part of that name means and why it's important:\n",
    "\n",
    "- **Resilient**: RDDs are fault-tolerant.  If a node in your cluster fails, the RDD can be reconstructed from other nodes.  Spark achieves this by tracking the lineage of each RDD, which is essentially a graph of the transformations applied to create it.  This lineage allows Spark to recompute lost partitions of the RDD without having to reload the entire dataset.\n",
    "\n",
    "- **Distributed**: RDDs are partitioned and distributed across multiple nodes in a cluster. This parallelization is key to Spark's performance, as it allows computations to be performed on different parts of the data simultaneously.  The data is split into chunks (partitions), and each partition can be processed on a different machine.\n",
    "\n",
    "- **Dataset**: RDDs represent a collection of data. This data can come from various sources, such as files (text files, CSV, Parquet, etc.), databases, or even other RDDs.  The data within an RDD can be of any data type (e.g., integers, strings, Python objects).\n",
    "\n",
    "Key Characteristics and Concepts related to RDDs:\n",
    "\n",
    "- **Immutability**: Once an RDD is created, it cannot be changed.  Transformations on an RDD create a new RDD. This immutability simplifies debugging and makes it easier to reason about the code.\n",
    "\n",
    "- **Lazy Evaluation**: Computations on RDDs are not performed immediately. Instead, Spark builds a plan of operations (the DAG - Directed Acyclic Graph) and executes it only when an action is triggered (e.g., collect, count, save). This lazy evaluation allows Spark to optimize the execution plan and avoid unnecessary computations.\n",
    "\n",
    "- **Transformations**: Operations that create new RDDs from existing ones are called transformations. Examples include ```map```, ```filter```, ```reduce```, ```groupBy```, ```join```, etc. These are lazy operations.\n",
    "\n",
    "- **Actions**: Operations that trigger the execution of the RDD computations and return a result to the driver program or write data to an external system are called actions. Examples include ```collect```, ```count```, ```first```, ```take```, ```reduce```, ```save```, etc. These are the operations that actually produce a result.\n",
    "\n",
    "- **Partitioning**: RDDs are divided into partitions, which are the basic units of parallelism. The number of partitions can be configured and significantly impacts performance.  Good partitioning ensures balanced workload distribution across the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d4ed09-c08b-437e-aa9e-390d6b78e95a",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6608f8-2135-403e-bf77-10aa34d45caf",
   "metadata": {},
   "source": [
    "- Create a list of numbers\n",
    "- Convert the list into an RDD and divide into 2 partitions\n",
    "- Apply a filter to retain *only* the odd numbers from the RDD\n",
    "- Retrieve the first 5 odd numbers from the filtered RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "270cb381-eeca-480c-8db4-509757ab1fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_list = range(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8ef3c10-54d7-4d66-983d-b4ec253e7eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(big_list, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c507727-76b2-4fe1-b29d-fd85329090f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "odds = rdd.filter(lambda x: x % 2 != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fd14c67-ce67-49b3-ba2e-372e6f67c0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odds.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1976a6e8-772e-4a00-9bed-daf383ad282d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (python39)",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
