{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resilient Distributed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD stands for Resilient Distributed Dataset. It's a fundamental data structure in Apache Spark, representing an immutable, distributed collection of data elements. Let's break down the key aspects:\n",
    "- **Resilient**: RDDs are fault-tolerant. If a node in your Spark cluster fails, the RDD can be reconstructed from other nodes. Spark achieves this through lineage tracking.  Lineage is a record of all the transformations applied to create an RDD. This lineage allows Spark to recompute lost partitions without having to reload the entire dataset.\n",
    "- **Distributed**: RDDs are partitioned and distributed across multiple nodes in a cluster. This parallelization is crucial for Spark's performance. The data is split into chunks (partitions), and each partition can be processed on a different machine concurrently.\n",
    "- **Dataset**: RDDs represent a collection of data. This data can come from various sources:\n",
    "  - Files (text files, CSV, Parquet, Avro, etc.)\n",
    "  - Databases (JDBC connections)\n",
    "  - Other RDDs\n",
    "  - In-memory collections\n",
    "- **Immutable**: Once an RDD is created, it cannot be changed. Transformations on an RDD create a new RDD. This immutability simplifies debugging and makes it easier to reason about the code.\n",
    "- **Lazy Evaluation**: Computations on RDDs are not performed immediately. Instead, Spark builds a plan of operations (a Directed Acyclic Graph or DAG) and executes it only when an action is triggered. This lazy evaluation allows Spark to optimize the execution plan and avoid unnecessary computations.\n",
    "\n",
    "**Key Concepts Related to RDDs**:\n",
    "- **Transformations**: Operations that create new RDDs from existing ones. Examples:\n",
    "  - `map`: Applies a function to each element.\n",
    "  - `filter`: Returns elements that satisfy a condition.\n",
    "  - `reduce`: Aggregates elements using a function.\n",
    "  - `groupBy`: Groups elements based on a key.\n",
    "  - `join`: Combines elements from two RDDs.\n",
    "- **Actions**: Operations that trigger the execution of RDD computations and return a result to the driver program or write data to an external system. Examples:\n",
    "  - `collect`: Returns all elements of the RDD to the driver program. (Use with caution for large datasets!)\n",
    "  - `count`: Returns the number of elements in the RDD.\n",
    "  - `first`: Returns the first element.\n",
    "  - `take`: Returns the first n elements.\n",
    "  - `saveAsTextFile`: Writes the RDD to a text file.\n",
    "- **Partitions**: RDDs are divided into partitions, which are the basic units of parallelism. The number of partitions can be configured and significantly impacts performance.  Good partitioning ensures balanced workload distribution across the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why are RDDs important?**\n",
    "\n",
    "RDDs provide a powerful abstraction for distributed data processing. They handle the complexities of data distribution, fault tolerance, and parallel execution, allowing developers to focus on the logic of their data processing tasks. While newer abstractions like DataFrames and Datasets are built on top of RDDs and offer more structured data handling and optimizations, understanding RDDs is still crucial for a deeper understanding of Spark's internals and for certain advanced use cases. They are the foundation upon which Spark's more modern data processing capabilities are built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/18 16:14:04 WARN Utils: Your hostname, Cesars-MBP.local resolves to a loopback address: 127.0.0.1; using 192.168.7.230 instead (on interface en0)\n",
      "25/02/18 16:14:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/18 16:14:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/18 16:14:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"ExperimentingWithRDDs\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a List of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits the given sentence, `text_for_list`, into a Python list of words based on whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_for_list = \"Spark makes life much easier and puts me in a good mood which makes Spark awesome!\".split(\" \")\n",
    "type(text_for_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spark', 'makes', 'life', 'much', 'easier', 'and', 'puts', 'me', 'in', 'a', 'good', 'mood', 'which', 'makes', 'Spark', 'awesome!']\n"
     ]
    }
   ],
   "source": [
    "print(text_for_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the List to an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parallelize**: converts `text_for_list` into an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_rdd = spark.sparkContext.parallelize(text_for_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect and Print RDD Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collect**: The `collect()` action gathers all the elements from the RDD back into a local Python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 0) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark\n",
      "makes\n",
      "life\n",
      "much\n",
      "easier\n",
      "and\n",
      "puts\n",
      "me\n",
      "in\n",
      "a\n",
      "good\n",
      "mood\n",
      "which\n",
      "makes\n",
      "Spark\n",
      "awesome!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "text_data = text_rdd.collect()\n",
    "\n",
    "for word in text_data:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Elements in the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count**: The action returns the total number of elements in the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Unique Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distinct**: The transformation creates a new RDD that contains only the unique elements from the `text_rdd`.\n",
    "\n",
    "***Note***: *Tranformations in Spark are lazy, i.e. they are not executed until an action is called.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_rdd.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recollecting and Printing the RDD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark\n",
      "makes\n",
      "life\n",
      "much\n",
      "easier\n",
      "and\n",
      "puts\n",
      "me\n",
      "in\n",
      "a\n",
      "good\n",
      "mood\n",
      "which\n",
      "makes\n",
      "Spark\n",
      "awesome!\n"
     ]
    }
   ],
   "source": [
    "text_data = text_rdd.collect()\n",
    "\n",
    "for word in text_data:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Print a Unique RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new RDD from an existing one by applying a transformation and then performing an action to view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "makes\n",
      "life\n",
      "puts\n",
      "a\n",
      "which\n",
      "Spark\n",
      "much\n",
      "easier\n",
      "awesome!\n",
      "in\n",
      "and\n",
      "me\n",
      "mood\n"
     ]
    }
   ],
   "source": [
    "text_unique_rdd = text_rdd.distinct()\n",
    "\n",
    "for word in text_unique_rdd.collect():\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Custom Function and Filter the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a custom logic transformation with `filter` to select a subset of data from an RDD and `collect` the data into a local list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordStartsWith(word, letter):\n",
    "    return word.startswith(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark', 'Spark']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_rdd.filter(lambda word: wordStartsWith(word, \"S\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a List of Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "num_list = [*range(1, 21)]\n",
    "print(num_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the List of Numbers into an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rdd = spark.sparkContext.parallelize(num_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map each Number to a Tuple of its Value and its Square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new RDD with `map` transformation that applies function to every element. For each number `n`, it creates a tuple `(n, n * n)`. The new RDD is a tuple of the form `(number, square)`.\n",
    "\n",
    "The `collect()` action gathers all the elements from the distributed RDD back to the driver as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "(2, 4)\n",
      "(3, 9)\n",
      "(4, 16)\n",
      "(5, 25)\n",
      "(6, 36)\n",
      "(7, 49)\n",
      "(8, 64)\n",
      "(9, 81)\n",
      "(10, 100)\n",
      "(11, 121)\n",
      "(12, 144)\n",
      "(13, 169)\n",
      "(14, 196)\n",
      "(15, 225)\n",
      "(16, 256)\n",
      "(17, 289)\n",
      "(18, 324)\n",
      "(19, 361)\n",
      "(20, 400)\n"
     ]
    }
   ],
   "source": [
    "num_squared_rdd = num_rdd.map(lambda n: (n, n * n))\n",
    "\n",
    "for element in num_squared_rdd.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Text Data to a Transformed Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the `map` transformation to an existing RDD, and for each word, a function produces a tuple into a new RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Spark', 'S', True)\n",
      "('makes', 'm', False)\n",
      "('life', 'l', False)\n",
      "('much', 'm', False)\n",
      "('easier', 'e', False)\n",
      "('and', 'a', False)\n",
      "('puts', 'p', False)\n",
      "('me', 'm', False)\n",
      "('in', 'i', False)\n",
      "('a', 'a', False)\n",
      "('good', 'g', False)\n",
      "('mood', 'm', False)\n",
      "('which', 'w', False)\n",
      "('makes', 'm', False)\n",
      "('Spark', 'S', True)\n",
      "('awesome!', 'a', False)\n"
     ]
    }
   ],
   "source": [
    "text_trained_rdd = text_rdd.map(lambda word: (word, word[0], word.startswith(\"S\")))\n",
    "for element in text_trained_rdd.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten Words into Characters and Retrieve a Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform a each word into its constituent characters and then get a sample of these characters.\n",
    "\n",
    "By applying the `flatMap` transformation, it flattens the list to a new RDD which consists of individual characters. The `take` action retrieves the first 10 characters from the flattened RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S', 'p', 'a', 'r', 'k', 'm', 'a', 'k', 'e', 's']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_rdd.flatMap(lambda word: list(word)).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Countries List and Convert to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = [(\"USA\", 96),  (\"India\", 68), (\"UK\", 86), (\"Germany\", 84), (\"Canada\", 82), (\"France\", 83), (\"Norway\", 81), (\"Australia\", 82), (\"Brazil\", 79), (\"Mexico\", 76)]\n",
    "\n",
    "countries_rdd = spark.sparkContext.parallelize(countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the Countries List by Country Name (Key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sortByKey()` transformation sorts the RDD based on the keys, and is brought back to the driver as a list with `collect()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Australia', 82)\n",
      "('Brazil', 79)\n",
      "('Canada', 82)\n",
      "('France', 83)\n",
      "('Germany', 84)\n",
      "('India', 68)\n",
      "('Mexico', 76)\n",
      "('Norway', 81)\n",
      "('UK', 86)\n",
      "('USA', 96)\n"
     ]
    }
   ],
   "source": [
    "sorted_countries = countries_rdd.sortByKey().collect()\n",
    "\n",
    "for element in sorted_countries:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse the Tuple Order and Sort in Descending Order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By altering the transformation with `sortByKey(False)`, the list is sorted in descending order, based on the value, after changing the position of the elements from `(country, value)` to `(value, country)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 'USA')\n",
      "(86, 'UK')\n",
      "(84, 'Germany')\n",
      "(83, 'France')\n",
      "(82, 'Canada')\n",
      "(82, 'Australia')\n",
      "(81, 'Norway')\n",
      "(79, 'Brazil')\n",
      "(76, 'Mexico')\n",
      "(68, 'India')\n"
     ]
    }
   ],
   "source": [
    "sorted_countries = countries_rdd.map(lambda c: (c[1], c[0])).sortByKey(False).collect()\n",
    "\n",
    "for element in sorted_countries:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
