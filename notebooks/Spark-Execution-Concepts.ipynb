{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Execution Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL is a core module of Apache Spark that provides capabilities for working with structured data. It allows you to query data using SQL or a DataFrame API, which is a programmatic interface for working with structured data, and integrates seamlessly with other Spark components, fitting perfectly within Spark's Unified Stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts in Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **SparkSession**:\n",
    "   - The SparkSession is the entry point to Spark SQL functionality. It's the central object you use to interact with Spark. Think of it as the starting point for your Spark application.\n",
    "   - It's responsible for creating DataFrames and executing SQL queries.\n",
    "   - It replaces the older ```SparkContext``` (which is still used for core Spark functionalities) and ```SQLContext``` (used in earlier versions of Spark SQL). ```SparkSession``` encapsulates both.\n",
    "\n",
    "2. **Application**:\n",
    "   - A Spark application is the overall program you write using Spark. It consists of a driver program and executors.\n",
    "   - The driver program is the main process that coordinates the execution of your Spark application. It creates the ```SparkSession```, defines the transformations and actions on the data, and schedules the tasks to be executed on the cluster.\n",
    "   - Executors are processes that run on worker nodes in the cluster. They execute the tasks assigned to them by the driver program.\n",
    "   - The ```SparkSession``` is created within the driver program and is associated with the application.\n",
    "\n",
    "3. **DataFrames**:\n",
    "   - A DataFrame is a distributed collection of data organized into named columns. It's conceptually similar to a table in a relational database1 or a Pandas DataFrame in Python, but it's distributed across the Spark cluster.\n",
    "   - DataFrames provide a structured view of the data, allowing you to perform operations using SQL queries or a DataFrame API.\n",
    "   - They are the primary abstraction for working with structured data in Spark SQL.\n",
    "  \n",
    "4. **SQL Queries**:\n",
    "   - Spark SQL allows you to execute SQL queries directly on DataFrames.\n",
    "   - This makes it easy for those familiar with SQL to work with Spark.\n",
    "   - Spark SQL supports a wide range of SQL features, including joins, aggregations, and window functions.\n",
    "\n",
    "5. **Catalyst Optimizer**:\n",
    "   - Spark SQL uses the Catalyst optimizer to optimize the execution of SQL queries and DataFrame operations.\n",
    "   - Catalyst analyzes the query and generates an optimized execution plan, which can significantly improve performance.\n",
    "\n",
    "6. **Tungsten Engine**:\n",
    "   - Tungsten is another performance optimization within Spark SQL.\n",
    "   - It focuses on improving memory management and code generation to make data processing more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/14 10:32:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession object in a PySpark application\n",
    "spark = SparkSession.builder.appName(\"TotalOrdersPerRegionCountry\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Import data file\n",
    "data_file = \"data/sales_records.csv\"\n",
    "\n",
    "# Load data file into a PySpark DataFrame\n",
    "sales_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sales_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations and Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select and Aggregation Actions from DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+---------------------+---------+\n",
      "|Region                           |Country              |Order ID |\n",
      "+---------------------------------+---------------------+---------+\n",
      "|Middle East and North Africa     |Azerbaijan           |535113847|\n",
      "|Central America and the Caribbean|Panama               |874708545|\n",
      "|Sub-Saharan Africa               |Sao Tome and Principe|854349935|\n",
      "|Sub-Saharan Africa               |Sao Tome and Principe|892836844|\n",
      "|Central America and the Caribbean|Belize               |129280602|\n",
      "|Europe                           |Denmark              |473105037|\n",
      "|Europe                           |Germany              |754046475|\n",
      "|Middle East and North Africa     |Turkey               |772153747|\n",
      "|Europe                           |United Kingdom       |847788178|\n",
      "|Asia                             |Kazakhstan           |471623599|\n",
      "+---------------------------------+---------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select specific columns from the DataFrame\n",
    "sales_df.select(\"Region\", \"Country\", \"Order ID\").show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-------------+------------+\n",
      "|Region                           |Country      |Total Orders|\n",
      "+---------------------------------+-------------+------------+\n",
      "|Sub-Saharan Africa               |Sudan        |623         |\n",
      "|Australia and Oceania            |New Zealand  |593         |\n",
      "|Europe                           |Vatican City |590         |\n",
      "|Europe                           |Malta        |589         |\n",
      "|Sub-Saharan Africa               |Mozambique   |589         |\n",
      "|Middle East and North Africa     |Tunisia      |584         |\n",
      "|Asia                             |Cambodia     |584         |\n",
      "|Central America and the Caribbean|Panama       |578         |\n",
      "|Sub-Saharan Africa               |Rwanda       |576         |\n",
      "|Sub-Saharan Africa               |Cote d'Ivoire|575         |\n",
      "+---------------------------------+-------------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Total Rows: 185\n"
     ]
    }
   ],
   "source": [
    "# Group by Region and Country columns and count the number of Order IDs\n",
    "count_sales_df = (sales_df.select(\"Region\", \"Country\", \"Order ID\")\n",
    "                  .groupBy(\"Region\", \"Country\").agg(count(\"Order ID\").alias(\"Total Orders\"))\n",
    "                  .orderBy(\"Total Orders\", ascending=False))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "count_sales_df.show(n=10, truncate=False)\n",
    "\n",
    "# Count the total number of rows in the DataFrame\n",
    "print(f\"Total Rows: {count_sales_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Narrow versus Wide Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Narrow Transformations** \n",
    "- **Definition:** A narrow transformation is one where each partition of the output RDD depends only on a single partition of the input RDD. In other words, the data required to compute a partition in the output RDD resides entirely within the corresponding partition of the input RDD.\n",
    "- **Characteristics:**\n",
    "  - **No data shuffling:** Do not require data to be shuffled between partitions or acorss the network.\n",
    "  - **Localized computation:** Each partition can be processed independently without needing data from other partitions.   \n",
    "  - **Examples:** ```map```, ```filter```, ```flatMap```, ```union```\n",
    "- **Benefits:**\n",
    "  - **Performance:** Narrow transformations are generally faster because they avoid the overhead of data shuffling.   \n",
    "  - **Fault tolerance:** If a partition is lost, it can be easily recomputed from the corresponding input partition.   \n",
    "\n",
    "**Wide Transformations**\n",
    "- **Definition:** A wide transformation is one where each partition of the output RDD may depend on multiple partitions of the input RDD. This means that data from different partitions needs to be combined or shuffled to produce the output.   \n",
    "- **Characteristics:**\n",
    "  - **Data shuffling:** Wide transformations require data to be shuffled across the network, which can be expensive.   \n",
    "  - **Global computation:** Computing a partition in the output RDD may require data from multiple input partitions.   \n",
    "  - **Examples:** ```groupByKey```, ```reduceByKey```, ```join```, ```distinct```, ```repartition```, ```intersection```, ```coalesce```\n",
    "  - **Considerations:**\n",
    "    - **Performance:** Wide transformations are generally slower than narrow transformations due to the data shuffling involved.   \n",
    "    - **Resource intensive:** Wide transformations can be more resource intensive as they may require significant network I/O and memory.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Difference: Data Shuffling**\n",
    "The fundamental difference between narrow and wide transformations lies in whether data shuffling is required. Narrow transformations do not involve shuffling, while wide transformations do. Data shuffling is the process of redistributing data across the cluster based on some key or condition. It is a costly operation that involves moving data over the network, which can be a bottleneck.   \n",
    "\n",
    "**Importance**\n",
    "Understanding the difference between narrow and wide transformations is crucial for optimizing Spark applications. By choosing appropriate transformations and minimizing data shuffling, you can significantly improve the performance of your Spark jobs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
